---
title: 'Statistical Computation and Visualization: Final Project'
author: "Issam Arabi, Eric Maeder"
date: "2022-12-20"
output: html_document
bibliography: /home/emaeder/Desktop/MyFiles/Stat_Comp_Project/references.bib
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


library(tidyverse)
library(dplyr)
library(locpol)
library(fANCOVA)
library(KernSmooth)
library(wavethresh)
library(ggplot2)
library(patchwork)
```

## Comparison of Local Regression in different R packages.

# Introduction
When applying statistical methods in R using built functions, it is common to be confronted to multiple possible functions and packages aiming to tackle the same issues. In this scenario, and especially when dealing with large amounts of data, it is crucial yet difficult to choose the better optimized one towards fulfilling one's goals. Optimization denotes here both a measure of performance in the results and in the computation time.

In this context, we aim to explore the different R packages aiming to implement local polynomial interpolation and their performance by applying them and timing their execution on specifically chosen data sets of different sizes and different structures to properly assess their versatility.

# Plan
In this report, we will be following the structure from the work of  @deng2011density conducting a similar analysis on density estimation. Effectively, we will start with a small introduction to local polynomial regression, followed by the description of the different R packages we are to examine. Then, we will at the computation time of the different functions when applied to datasets of increasing sizes. Next, we will measure the performance of the considered functions when dealing with data sets with more challenging structures. Afterwards, we will discuss the results, and lastly, we will summarize our results and conclude.

# Local Polynomial Regression
Local Polyinomial regression, most widely known through the Locally Estimated Scatterplot Smoothing (LOESS) method, is a powerful prediction tool when observing bounded data sets with non-trivial distributions. It is a non-parametric method, which allows it to be versatile and easy to use. 

When observing response values $Y_1, Y_2, ...$ at points $X_1, X_2, ...$, the aim of local polynomial regression is to assign at each point X the value corresponding to $\underset{\beta \in \mathbb{R}^{p+1}}{argmin} \sum_{i=1}^n [Y_i - \beta_0 - \beta_1(X_i - x) - ... - \beta_p (X_i - x)^p]^2 K(\frac{X_i - x}{h})$, where p is the order of the fitted polynomial, K is a kernel function (basically a weighting function), and h is a bandwidth parameter allowing to choose how strong the influence of distant observations on the regression should be.  
As we can see, this is a least squares problem, and although the solution is long and unexciting to produce, it comes down to computing a gradient and solving a relatively simple system of equations.

Then, the resulting polynomials are combined and smoothed to obtain a curve spanning the whole considered region. In practice, polynomial degrees p < 4 are mostly used due to the increasingly computational challenge, which is also the approach we will adopt in this report.

# Local Polynomial Regression Packages

In this section, we give a quick overview of the considered functions and their parameters.

+ **stats::loess**, @stats, from its documentation, fits a locally polynomial surface determined by one or more numerical predictors, using local fitting. its typical call is as follow: 

loess(formula, data, weights, subset, na.action, model = FALSE,  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;      span = 0.75, enp.target, degree = 2,  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;      parametric = FALSE, drop.square = FALSE, normalize = TRUE,  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;      family = c("gaussian", "symmetric"),  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;      method = c("loess", "model.frame"),  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;      control = loess.control(...), ...)
      
But in our case, we will be working on fully-specified data sets with no missing data and no prior information about value importance, so we will only use the following:

loess(formula, (data),  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;      span = 0.75, degree = 2,  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;      family = "gaussian"  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;      method = "loess")
      
where we give our response and explanatory variables in formula, span controls the bandwidth, degree is self-explanatory and family specifies we want a least-squares approach. 
The span parameter works as follows: when given a value smaller than 1, each local regression is fitted using the proportion of observations equal to the span value into account, with tricubic weighting (1- (distance to point/maximal distance to considered points)^3)^3. 

This approach prevents cases of badly spread data where no observation is in the considered bandwidth when fitting a local regression, making it unstable, at the cost of computing the distance to each data point. Moreover, this makes it difficult to compare to other functions, as we cannot give them exactly the same inputs.

+ **locpol::locpol**, @locpol, follows the steps of the local polynomial regression described in the previous section. Its call is as follows: 

locpol(formula,data,weig=rep(1,nrow(data)),bw=NULL,kernel=EpaK,deg=1,  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    xeval=NULL,xevalLen=100)

again, we will not use all parameters for comparison in this report, although they offer room for versatility. Therefore, we will mostly use:

locpol(formula,(data),bw=NULL,kernel=EpaK,deg=1)

where formula, data and deg are identical to the loess function. The bw parameter is the bandwidth, and the kernel is the kernel function, Epanechnikov by default, but can be gaussian, uniform,...
This follows closely the theoretical approach we chose.

+ **KernSmooth::locpoly**, @kernsmooth, aims to fit a local polynomial with kernel weighting to estimate a regression function. It can also be used towards density estimation and their derivatives. We call it as follows:

locpoly(x, y, drv = 0L, degree, kernel = "normal",  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    bandwidth, gridsize = 401L, bwdisc = 25,  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    range.x, binned = FALSE, truncate = TRUE)

Here, x,y are the explanatory and response variables, drv concerns derivative estimations, kernel and bandwidth are self-explanatory, gridsize represents the number of equidistant points over which we want to estimate the fct, and the last parameters are used to speed up computations when the data is pre-processed. This approach is pretty similar to locpol, and as such to our theoretical version. The similarity in input parameters makes it easy to compare both of them.

+ **fANCOVA::loess.as**, @fancova, performs local polynomial regression with automatic bandwidth selection using either cross-validation or AIC as criterion. It also has an option to select it manually, which we will use towards comparison. below the usage:

loess.as(x, y, degree = 1, criterion = c("aicc", "gcv"),  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;	family = c("gaussian", "symmetric"), user.span = NULL,  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;	plot = FALSE, ...)
		
where x,y,degree, criterion are self-explanatory. We will use here the gaussian family corresponding to the Least Squares approach, and user.span is a manual smoothing parameter selection (bandwidth value e.g.). plot is an option to generate a visualization of the results.

# Other more specific local regressions

+ **caret::knnreg** ,@caret, and other k-nearest neighbors approaches, are a substitute to the bandwidth approach, making sure that isolates on the x-axis do not have too much impact on the results by always taking at least k observations into account. 

+ **spgwr::gwr**, @spgwr, performs a geographically weighted regression; a special type of local regression applicable to spatially ordered data, taking into account the distance of each observations into account when fitting a local estimate.

# Comparison of Computation times

```{r}

X4 <- seq(from = -5, to = 5, by= 0.01)
d4 <- dnorm(X4)
df4 <-data.frame(X4,d4)
lo <- 0
lp <- 0
loas <- 0
loly <- 0

for (i in 1:1000){
  t <- Sys.time()
  loess(d4 ~ X4, data = df4, family = "gaussian", method = "loess", span = 0.075)
  lo <- lo + Sys.time() - t
}

for (i in 1:1000){
  t <- Sys.time()
  locpol(d4 ~ X4, data = df4, bw = 0.025, kernel = gaussK, deg = 2)
  lp <- lp + Sys.time() - t
}

for (i in 1:1000){
  t <- Sys.time()
  loess.as(df4$X4, df4$d4, degree = 2, criterion = "aicc", family = "gaussian")
  loas <- loas + Sys.time() - t
}

for (i in 1:1000){
  t <- Sys.time()
  locpoly(df4$X4, df4$d4, degree = 2, kernel = "normal", bandwidth = 0.025, gridsize = 1001)
  loly <-loly  + Sys.time() - t
}




```

To compare computation times for the functions, we run each of them for 1000 times and compare the average runtime. Below are the results.

```{r}

freq_data <- tibble(loess=lo/1000, locpol=lp/1000, loess.as=loas/1000, locpoly=loly/1000)

print(freq_data)

```

From this data, we can see that loess.as takes much more time than the rest of the functions, followed by locpol, then loess. The most computationally efficient function was locpoly. This is due to the loess.as function calculating its optimal bandwidth internally, requiring the application of a selection algorithm, such as cross validation, or more generally a K-folds approach, for example. This does require extra time, but has the advantage of combining choosing an efficient span parameter and applying it at once. A more valid comparison would be to time a K-folds approach on the datasets to choose either a bandwidth or a span parameter, and add its execution time to the other functions.

# Comparison of approximation residuals

Now, we delve into the comparison of the fit produced by the different functions, applied on the same datasets. To do so, we use two families of different datasets: one stemming from the gaussian distribution added to a sinusoidal factor, and one originating from the claw distribution noised @wavethresh. For both families, we look at samples of size $10^2,10^3,10^4$ and $10^5$. On each of those, we apply the four considered functions and we calculate the $R^2$ value, representing the goodness of fit (coefficient of determination). This allows for comparison both between different functions on the same datasets and for the same function on different sizes of datasets.  
Worth to keep in mind is that two of the functions take a span parameter into account, denoting a percentage of the datapoints to consider, and the two others a bandwidth parameter. This implies the two first will not be meaningfully comparable to the latters, as we do not run any algorithm, such as cross-validation, to choose a bandwidth parameter. For the span parameter, since such an algorithm is already implemented in the loess.as function, we run the loess function with the resulting parameter to have comparatively similar performances.  
Since locpol function is un-applicable to datasets too large, we set the resulting R2 to 0 on the last plot.  
We output the different fit functions, tables of the $R^2$ coefficients, and lastly a plot of the coefficients of determination on each dataset size.

Let us start with the smaller datasets:

``` {r c2, echo = FALSE}
X3 <- seq(from = -5, to = 5, by= 0.1)
X4 <- seq(from = -5, to = 5, by= 0.01)
X5 <- seq(from = -5, to = 5, by= 0.001)
X6 <- seq(from = -5, to = 5, by= 0.0001)
X7 <- seq(from = -5, to = 5, by= 0.00001) # X-axis evaluation points

Y3 <- dclaw(X3) # claw distribution
n3 <- rnorm(X3, sd = 0.1) # gaussian noise
Y3 <- Y3 + n3

# applying the functions:
daf3 <- data.frame(X3,Y3)
Lo3c <- loess(Y3 ~ X3, data = daf3, family = "gaussian", method = "loess", span = 0.587) # Lo3c$residuals
Lpc3 <- locpol(Y3 ~ X3, data = daf3, bw = 0.25, kernel = gaussK, deg = 2) # Lpc3$residuals
Loasc3 <- loess.as(daf3$X3, daf3$Y3, degree = 2, criterion = "aicc", family = "gaussian") # Loasc3$residuals
LoLyc3 <- locpoly(daf3$X3, daf3$Y3, degree = 2, kernel = "normal", bandwidth = 0.25, gridsize = 101) # only gives results


# R^2 coefficients
#Loess
SSres <- sum((Y3 - Lo3c$fitted)^2)
SStot <- sum((Y3 - mean(Y3))^2)
R2Lo3 <- 1 - SSres/SStot
#Locpol
SSres <- sum((Y3 - Lpc3$mf[1])^2)
SStot <- sum((Y3 - mean(Y3))^2)
R2Lp3 <- 1 - SSres/SStot
#Loess.as
SSres <- sum((Y3 - Loasc3$fitted)^2)
SStot <- sum((Y3 - mean(Y3))^2)
R2Loas3 <- 1 - SSres/SStot
#locpoly
SSres <- sum((Y3 - LoLyc3$y)^2)
SStot <- sum((Y3 - mean(Y3))^2)
R2Loly3 <- 1 - SSres/SStot

#plot
fitted_loc <- Lo3c$fitted
fitted_lp <- Lpc3$mf[1]
fitted_loas <- Loasc3$fitted
fitted_Loly <- LoLyc3$y


pc2 <- ggplot() + geom_point(mapping = aes(x = X3, y = fitted_loc, color = "loess")) + 
  geom_point(mapping = aes(x = X3, y = unlist(fitted_lp), color = "locpol")) + 
  geom_point(mapping = aes(x = X3, y = fitted_loas, color = "loess.as")) + 
  geom_point(mapping = aes(x = X3, y = fitted_Loly, color = "locpoly")) + 
  scale_color_manual(name = "Regression Function",
                     values = c( "loess" = "blue", "locpol" = "red", "loess.as" = "green", "locpoly" = "purple"),
                     labels = c("loess", "locpol", "loess.as","locpoly")) + 
  labs(y = "fit") +
  ggtitle("Claw distribution, 10^2 data points")

pc2
```

For now, we cannot really see the claws emerging from the distribution, but that is only due to the inaccuracy induced by the small sample size, as the locpol function produces an R^2 of 1, meaning it approximates the function perfectly. The others underfit a bit, leading to smaller values and worse approximation. However, that might not be a bad result, as we are a priori with so little data not sure we should fit the data so precisely in fear of overfitting. 

Let us look at what changes when we consider a superior order of magnitude:

```{r c3, echo = FALSE}
## 1001
#Distribution
Y4 <- dclaw(X4)
n4 <- rnorm(X4, sd = 0.1)
Y4 <- Y4 + n4

# Application
daf4 <- data.frame(X4,Y4)
Lo4c <- loess(Y4 ~ X4, data = daf4, family = "gaussian", method = "loess", span = 0.211) # Lo3c$residuals
Lpc4 <- locpol(Y4 ~ X4, data = daf4, bw = 0.25, kernel = gaussK, deg = 2) # Lpc3$residuals
Loasc4 <- loess.as(daf4$X4, daf4$Y4, degree = 2, criterion = "aicc", family = "gaussian") # Loasc3$residuals
LoLyc4 <- locpoly(daf4$X4, daf4$Y4, degree = 2, kernel = "normal", bandwidth = 0.25, gridsize = 1001) # only gives results


# R^2 coefficients
#Loess
SSres <- sum((Y4 - Lo4c$fitted)^2)
SStot <- sum((Y4 - mean(Y4))^2)
R2Lo4 <- 1 - SSres/SStot
#Locpol
SSres <- sum((Y4 - Lpc4$mf[1])^2)
SStot <- sum((Y4 - mean(Y4))^2)
R2Lp4 <- 1 - SSres/SStot
#Loess.as
SSres <- sum((Y4 - Loasc4$fitted)^2)
SStot <- sum((Y4 - mean(Y4))^2)
R2Loas4 <- 1 - SSres/SStot
#locpoly
SSres <- sum((Y4 - LoLyc4$y)^2)
SStot <- sum((Y4 - mean(Y4))^2)
R2Loly4 <- 1 - SSres/SStot

#plot
fitted_loc <- Lo4c$fitted
fitted_lp <- Lpc4$mf[1]
fitted_loas <- Loasc4$fitted
fitted_Loly <- LoLyc4$y


pc3 <- ggplot() + geom_point(mapping = aes(x = X4, y = fitted_loc, color = "loess")) + 
  geom_point(mapping = aes(x = X4, y = unlist(fitted_lp), color = "locpol")) + 
  geom_point(mapping = aes(x = X4, y = fitted_loas, color = "loess.as")) + 
  geom_point(mapping = aes(x = X4, y = fitted_Loly, color = "locpoly")) + 
  scale_color_manual(name = "Regression Function",
                     values = c( "loess" = "blue", "locpol" = "red", "loess.as" = "green", "locpoly" = "purple"),
                     labels = c("loess", "locpol", "loess.as","locpoly")) + 
  labs(y = "fit") +
  ggtitle("Claw distribution, 10^3 data points") # loess is under loess.as
pc3
```

Again, the locpol function interpolates the data perfectly, and we can indeed see the claws appearing on the plot. The others are still underfitting by quite a lot this time, leading to $R^2$ coefficients around 0.65 for all the three others. Interesting to note is that the locpoly and the locpol function perform differently by a lot even though we input the same parameters.  
Interesting is also that the two functions with an algorithmically-computed span from the loess.as function underfit quite badly. This hints that this specific method is not sensible enough to local variations, maybe because of the use of a too slow-decreasing kernel when taking the span parameter into account.  
The great performance of the locpol function however ends there, as we are prevented from using it on bigger dataset by a 'maxevalpts' parameter, limiting the maximum size we can input. Let us look at the two last density plots together, as they contain similar results.

```{r c45, echo = FALSE}
#distribution
Y5 <- dclaw(X5)
n5 <- rnorm(X5, sd = 0.1)
Y5 <- Y5 + n5

# Application
daf5 <- data.frame(X5,Y5)
Lo5c <- loess(Y5 ~ X5, data = daf5, family = "gaussian", method = "loess", span = 0.05) # Lo3c$residuals
#Lpc5 <- locpol(Y5 ~ X5, data = daf3, bw = 0.25, kernel = gaussK, deg = 2) # Lpc3$residuals
Loasc5 <- loess.as(daf5$X5, daf5$Y5, degree = 2, criterion = "aicc", family = "gaussian") # Loasc3$residuals
LoLyc5 <- locpoly(daf5$X5, daf5$Y5, degree = 2, kernel = "normal", bandwidth = 0.25, gridsize = 10001) # only gives results

# R^2 Coefficients
#Loess
SSres <- sum((Y5 - Lo5c$fitted)^2)
SStot <- sum((Y5 - mean(Y5))^2)
R2Lo5 <- 1 - SSres/SStot
#Loess.as
SSres <- sum((Y5 - Loasc5$fitted)^2)
SStot <- sum((Y5 - mean(Y5))^2)
R2Loas5 <- 1 - SSres/SStot
#locpoly
SSres <- sum((Y5 - LoLyc5$y)^2)
SStot <- sum((Y5 - mean(Y5))^2)
R2Loly5 <- 1 - SSres/SStot

#plot
fitted_loc <- Lo5c$fitted
fitted_loas <- Loasc5$fitted
fitted_Loly <- LoLyc5$y


pc4 <- ggplot() + geom_point(mapping = aes(x = X5, y = fitted_loc, color = "loess")) + 
  geom_point(mapping = aes(x = X5, y = fitted_loas, color = "loess.as")) + 
  geom_point(mapping = aes(x = X5, y = fitted_Loly, color = "locpoly")) + 
  scale_color_manual(name = "Regression Function",
                     values = c( "loess" = "blue",  "loess.as" = "green", "locpoly" = "purple"),
                     labels = c("loess", "loess.as","locpoly")) + 
  labs(y = "fit") +
  ggtitle("Claw distribution, 10^4 data points") # loess is under loess.as

# X6
# Distribution
Y6 <- dclaw(X6)
n6 <- rnorm(X6, sd = 0.1)
Y6 <- Y6 + n6

# Application
daf6 <- data.frame(X6,Y6)
Lo6c <- loess(Y6 ~ X6, data = daf6, family = "gaussian", method = "loess", span = 0.05) # Lo3c$residuals with span obtained from loess.as

Loasc6 <- loess.as(daf6$X6, daf6$Y6, degree = 2, criterion = "aicc", family = "gaussian") # Loasc3$residuals
LoLyc6 <- locpoly(daf6$X6, daf6$Y6, degree = 2, kernel = "normal", bandwidth = 0.25, gridsize = 100001) # only gives results

# R^2 Coefficients
# Loess
SSres <- sum((Y6 - Lo6c$fitted)^2)
SStot <- sum((Y6 - mean(Y5))^2)
R2Lo6 <- 1 - SSres/SStot
#loess.as
SSres <- sum((Y6 - Loasc6$fitted)^2)
SStot <- sum((Y6 - mean(Y6))^2)
R2Loas6 <- 1 - SSres/SStot
#locpoly
SSres <- sum((Y6 - LoLyc6$y)^2)
SStot <- sum((Y6 - mean(Y6))^2)
R2Loly6 <- 1 - SSres/SStot

#plot
fitted_loc2 <- Lo6c$fitted
#fitted_lp <- Lpc5$mf[1]
fitted_loas2 <- Loasc6$fitted
fitted_Loly2 <- LoLyc6$y


pc5 <- ggplot() + geom_point(mapping = aes(x = X6, y = fitted_loc2, color = "loess")) + 
  geom_point(mapping = aes(x = X6, y = fitted_loas2, color = "loess.as")) + 
  geom_point(mapping = aes(x = X6, y = fitted_Loly2, color = "locpoly")) + 
  scale_color_manual(name = "Regression Function",
                     values = c( "loess" = "blue",  "loess.as" = "green", "locpoly" = "purple"),
                     labels = c("loess", "loess.as","locpoly")) + 
  labs(y = "fit") +
  ggtitle("Claw distribution, 10^5 data points") # loess is under loess.as
# at this point basically, loess does the same as loess.as, loess.as is great, locpoly underfits a lot (too big bw most likely)
pc4 
pc5
```

On the above two plots, we see loess and loess.as have shaped the target claw function quite nicely, which is appropriate, as we noised the data using a mean zero gaussian function. Towards comprehension, let us have a look at the given dataset for the last computations (size $10^5$):

```{r claw, echo = FALSE}
plot(X6,Y6,main = "noised claw distribution on 10^5 points")
```

Now, let us look at the $R^2$ coefficients across the dataset sizes under the form of a table and a plot:

```{r r2c, echo = FALSE}
tab <- matrix(c(R2Lo3, R2Lp3, R2Loas3, R2Loly3, R2Lo4, R2Lp4, R2Loas4, R2Loly4,R2Lo5, "-", R2Loas5, R2Loly5, R2Lo6, "-", R2Loas6, R2Loly6), ncol=4, byrow=TRUE)
colnames(tab) <- c('Loess','locpol','loess.as',"locpoly")
rownames(tab) <- c('10^2','10^3','10^4',"10^5")
tab <- as.table(tab)

# plotting r^2 coefficients
locr2 <- c(R2Lo3,R2Lo4,R2Lo5,R2Lo6)
lpr2 <- c(R2Lp3,R2Lp4,0,0)
loasr2 <- c(R2Loas3,R2Loas4,R2Loas5,R2Loas6)
locyr2 <- c(R2Loly3,R2Loly4,R2Loly5,R2Loly6)
X <- c(10^2,10^3,10^4,10^5)

pr2c <- ggplot() + geom_point(mapping = aes(x = log10(X), y = locr2, color = "loess")) + 
  geom_point(mapping = aes(x = log10(X), y = lpr2, color = "locpol")) + 
  geom_point(mapping = aes(x = log10(X), y = loasr2, color = "loess.as")) + 
  geom_point(mapping = aes(x = log10(X), y = locyr2, color = "locpoly")) + 
  scale_color_manual(name = "Regression Function",
                     values = c( "loess" = "blue", "locpol" = "red", "loess.as" = "green", "locpoly" = "purple"),
                     labels = c("loess", "locpol", "loess.as","locpoly")) + 
  labs(y = "R2") +
  ggtitle("R2 coefficients applied to claw distribution, for different functions, and data size")
tab
pr2c

```

First looking at the table, we see loess and loess.as function similarly given the same parameters, meaning we can exchange one for the other, resultswise, depending on whether we know the span we want to use or not. Indeed, computing this parameter takes of course time. Indeed, Cross-validation has an $O(n^2)$ complexity, reduced to $O(Kn)$ when considering K-folds cross validation, where n is the sample size. For the two bandwidth functions, locpol seems whenever applicable, a better choice compared to the locpoly, as it yields better results altogether. Caution is however needed when interpreting these results, as although a coefficient of determination equal to unity means the data is perfectly interpolated, this does not mean the algorithm performs well when dealing with noised data, as it means the locpol function is overfitting the data points. In this case however, this is preferable, as it is the only function that shaped the existing claws when applied on datasets of small size.

Looking at the plot now, we see the loess and loess.as actually increase their coefficients of determination as the sample size grows. Indeed, it seems it requires a lot of data to 'convince' the loess.as algorithm to reduce its span parameter enough not to underfit anymore. They obtain here better results than the locpoly function, but this entirely depends on the bandwidth parameter, that we would normally tune finely by applying an algorithm to choose it. As an example, we see the tendency to underfit disappears as expected when reducing the bandwidth, here by dividing it by 10:

```{r locpoly, echo = FALSE}
LoLyc5 <- locpoly(daf5$X5, daf5$Y5, degree = 2, kernel = "normal", bandwidth = 0.025, gridsize = 10001) # only gives results
fitted_loc <- Lo5c$fitted
fitted_loas <- Loasc5$fitted
fitted_Loly <- LoLyc5$y
pc4
```

Now moving to a different family of dataset, we generate this time a gaussian random variable to which we add small sinusoidal components, aiming to generate softer waves than the claw function. We expect here the functions to perform better, as the local variations should be smoother. We also noise them identically to the claw dataset. This time, we use a smaller bandwidth for bigger datasets for the locpoly function, as the previous one was definitely too big for the bigger datasets, as a higher x-axis density leads to more local samples and thus this reduction is relevant. 

With this in mind, we now once again look at the different results, starting with order 100, moving to 10^5.  
First, we look at original curve we are trying to interpolate on $10^4$ points, without the noise: 

```{r target, echo = FALSE}
Y5 <- dnorm(X5) # normal distribution
s5 <- 1/10*sin(4*X5) # sinusoidal component
Y5 <- Y5 + s5
plot(X5,Y5, main = "Gaussian distribution with sinusoidal component")
```

As we can see, the target is still wiggly, but the second derivative varies slower than in the previous example. Let us see how the different functions perform;
```{r s2, echo = FALSE}
#101:

Y3 <- dnorm(X3) # normal distribution
s3 <- 1/10*sin(4*X3) # sinusoidal noise
n3 <- rnorm(X3, sd = 0.05) # gaussian noise
Y3 <- Y3 + s3 + n3

# applying the functions:
daf3 <- data.frame(X3,Y3)
Lo3c <- loess(Y3 ~ X3, data = daf3, family = "gaussian", method = "loess", span = 0.587) # Lo3c$residuals
Lpc3 <- locpol(Y3 ~ X3, data = daf3, bw = 0.25, kernel = gaussK, deg = 2) # Lpc3$residuals
Loasc3 <- loess.as(daf3$X3, daf3$Y3, degree = 2, criterion = "aicc", family = "gaussian") # Loasc3$residuals
LoLyc3 <- locpoly(daf3$X3, daf3$Y3, degree = 2, kernel = "normal", bandwidth = 0.25, gridsize = 101) # only gives results


# R^2 coefficients
#Loess
SSres <- sum((Y3 - Lo3c$fitted)^2)
SStot <- sum((Y3 - mean(Y3))^2)
R2Lo3 <- 1 - SSres/SStot
#Locpol
SSres <- sum((Y3 - Lpc3$mf[1])^2)
SStot <- sum((Y3 - mean(Y3))^2)
R2Lp3 <- 1 - SSres/SStot
#Loess.as
SSres <- sum((Y3 - Loasc3$fitted)^2)
SStot <- sum((Y3 - mean(Y3))^2)
R2Loas3 <- 1 - SSres/SStot
#locpoly
SSres <- sum((Y3 - LoLyc3$y)^2)
SStot <- sum((Y3 - mean(Y3))^2)
R2Loly3 <- 1 - SSres/SStot

#plot
fitted_loc <- Lo3c$fitted
fitted_lp <- Lpc3$mf[1]
fitted_loas <- Loasc3$fitted
fitted_Loly <- LoLyc3$y


pc2 <- ggplot() + geom_point(mapping = aes(x = X3, y = fitted_loc, color = "loess")) + 
  geom_point(mapping = aes(x = X3, y = unlist(fitted_lp), color = "locpol")) + 
  geom_point(mapping = aes(x = X3, y = fitted_loas, color = "loess.as")) + 
  geom_point(mapping = aes(x = X3, y = fitted_Loly, color = "locpoly")) + 
  scale_color_manual(name = "Regression Function",
                     values = c( "loess" = "blue", "locpol" = "red", "loess.as" = "green", "locpoly" = "purple"),
                     labels = c("loess", "locpol", "loess.as","locpoly")) + 
  labs(y = "fit") +
  ggtitle("Gaussian with sinusoidal noise function, 10^2 data points")
pc2
```

On the first graph, we can see the span approaches underfit the curve quite strongly, the locpoly function gives a good idea of the underlying curve, and the locpol function , if maybe too wiggly, also gives good results back. This is similar to what happened in the claw distribution case.  
Let us observe the evolution of the regressions, as the number of data points increases:
```{r ps3, echo = FALSE}
## 1001
#Distribution
Y4 <- dnorm(X4) # normal distribution
s4 <- 1/10*sin(4*X4) # sinusoidal component
n4 <- rnorm(X4, sd = 0.05) # gaussian noise
Y4 <- Y4 + s4 + n4

# Application
daf4 <- data.frame(X4,Y4)
Lo4c <- loess(Y4 ~ X4, data = daf4, family = "gaussian", method = "loess", span = 0.211) # Lo3c$residuals
Lpc4 <- locpol(Y4 ~ X4, data = daf4, bw = 0.25, kernel = gaussK, deg = 2) # Lpc3$residuals
Loasc4 <- loess.as(daf4$X4, daf4$Y4, degree = 2, criterion = "aicc", family = "gaussian") # Loasc3$residuals
LoLyc4 <- locpoly(daf4$X4, daf4$Y4, degree = 2, kernel = "normal", bandwidth = 0.25, gridsize = 1001) # only gives results


# R^2 coefficients
#Loess
SSres <- sum((Y4 - Lo4c$fitted)^2)
SStot <- sum((Y4 - mean(Y4))^2)
R2Lo4 <- 1 - SSres/SStot
#Locpol
SSres <- sum((Y4 - Lpc4$mf[1])^2)
SStot <- sum((Y4 - mean(Y4))^2)
R2Lp4 <- 1 - SSres/SStot
#Loess.as
SSres <- sum((Y4 - Loasc4$fitted)^2)
SStot <- sum((Y4 - mean(Y4))^2)
R2Loas4 <- 1 - SSres/SStot
#locpoly
SSres <- sum((Y4 - LoLyc4$y)^2)
SStot <- sum((Y4 - mean(Y4))^2)
R2Loly4 <- 1 - SSres/SStot

#plot
fitted_loc <- Lo4c$fitted
fitted_lp <- Lpc4$mf[1]
fitted_loas <- Loasc4$fitted
fitted_Loly <- LoLyc4$y


pc3 <- ggplot() + geom_point(mapping = aes(x = X4, y = fitted_loc, color = "loess")) + 
  geom_point(mapping = aes(x = X4, y = unlist(fitted_lp), color = "locpol")) + 
  geom_point(mapping = aes(x = X4, y = fitted_loas, color = "loess.as")) + 
  geom_point(mapping = aes(x = X4, y = fitted_Loly, color = "locpoly")) + 
  scale_color_manual(name = "Regression Function",
                     values = c( "loess" = "blue", "locpol" = "red", "loess.as" = "green", "locpoly" = "purple"),
                     labels = c("loess", "locpol", "loess.as","locpoly")) + 
  labs(y = "fit") +
  ggtitle("Gaussian with sinusoidal noise function, 10^3 data points") # loess is under loess.as
pc3
```

Here, we are still fitting the locpol and the locpoly functions with the same input parameters, but we see once again a difference. Indeed, it seems the locpol function has a tendency to overfit quite badly in this case, which gives us a noisy and rough resulting underlying curve, whereas the locpoly function and the two span approaches give a better result. Noticeable is the small underfit of the loess function, possibly due to the approximation of the span parameter we gave it, instead of the exact number (order of $10^{-3}$). 

```{r ps4, echo = FALSE}
# 10001 

#distribution
Y5 <- dnorm(X5) # normal distribution
s5 <- 1/10*sin(4*X5) # sinusoidal component
n5 <- rnorm(X5, sd = 0.05) # gaussian noise
Y5 <- Y5 + s5 + n5

# Application
daf5 <- data.frame(X5,Y5)
Lo5c <- loess(Y5 ~ X5, data = daf5, family = "gaussian", method = "loess", span = 0.05) # Lo3c$residuals
#Lpc5 <- locpol(Y5 ~ X5, data = daf3, bw = 0.25, kernel = gaussK, deg = 2) # Lpc3$residuals
Loasc5 <- loess.as(daf5$X5, daf5$Y5, degree = 2, criterion = "aicc", family = "gaussian") # Loasc3$residuals
LoLyc5 <- locpoly(daf5$X5, daf5$Y5, degree = 2, kernel = "normal", bandwidth = 0.025, gridsize = 10001) # only gives results

# R^2 Coefficients
#Loess
SSres <- sum((Y5 - Lo5c$fitted)^2)
SStot <- sum((Y5 - mean(Y5))^2)
R2Lo5 <- 1 - SSres/SStot
#Loess.as
SSres <- sum((Y5 - Loasc5$fitted)^2)
SStot <- sum((Y5 - mean(Y5))^2)
R2Loas5 <- 1 - SSres/SStot
#locpoly
SSres <- sum((Y5 - LoLyc5$y)^2)
SStot <- sum((Y5 - mean(Y5))^2)
R2Loly5 <- 1 - SSres/SStot

#plot
fitted_loc <- Lo5c$fitted
fitted_loas <- Loasc5$fitted
fitted_Loly <- LoLyc5$y


pc4 <- ggplot() + geom_point(mapping = aes(x = X5, y = fitted_loc, color = "loess")) + 
  geom_point(mapping = aes(x = X5, y = fitted_loas, color = "loess.as")) + 
  geom_point(mapping = aes(x = X5, y = fitted_Loly, color = "locpoly")) + 
  scale_color_manual(name = "Regression Function",
                     values = c( "loess" = "blue",  "loess.as" = "green", "locpoly" = "purple"),
                     labels = c("loess", "loess.as","locpoly")) + 
  labs(y = "fit") +
  ggtitle("Gaussian with sinusoidal noise function, 10^4 data points") # loess is under loess.as

```

```{r ps5, echo = FALSE}
# X6
# Distribution
Y6 <- dnorm(X6) # normal distribution
s6 <- 1/10*sin(4*X6) # sinusoidal component
n6 <- rnorm(X6, sd = 0.05) # gaussian noise
Y6 <- Y6 + s6 + n6

# Application
daf6 <- data.frame(X6,Y6)
Lo6c <- loess(Y6 ~ X6, data = daf6, family = "gaussian", method = "loess", span = 0.05) # Lo3c$residuals with span obtained from loess.as

Loasc6 <- loess.as(daf6$X6, daf6$Y6, degree = 2, criterion = "aicc", family = "gaussian") # Loasc3$residuals
LoLyc6 <- locpoly(daf6$X6, daf6$Y6, degree = 2, kernel = "normal", bandwidth = 0.025, gridsize = 100001) # only gives results

# R^2 Coefficients
# Loess
SSres <- sum((Y6 - Lo6c$fitted)^2)
SStot <- sum((Y6 - mean(Y5))^2)
R2Lo6 <- 1 - SSres/SStot
#loess.as
SSres <- sum((Y6 - Loasc6$fitted)^2)
SStot <- sum((Y6 - mean(Y6))^2)
R2Loas6 <- 1 - SSres/SStot
#locpoly
SSres <- sum((Y6 - LoLyc6$y)^2)
SStot <- sum((Y6 - mean(Y6))^2)
R2Loly6 <- 1 - SSres/SStot

#plot
fitted_loc2 <- Lo6c$fitted
#fitted_lp <- Lpc5$mf[1]
fitted_loas2 <- Loasc6$fitted
fitted_Loly2 <- LoLyc6$y


pc5 <- ggplot() + geom_point(mapping = aes(x = X6, y = fitted_loc2, color = "loess")) + 
  geom_point(mapping = aes(x = X6, y = fitted_loas2, color = "loess.as")) + 
  geom_point(mapping = aes(x = X6, y = fitted_Loly2, color = "locpoly")) + 
  scale_color_manual(name = "Regression Function",
                     values = c( "loess" = "blue",  "loess.as" = "green", "locpoly" = "purple"),
                     labels = c("loess", "loess.as","locpoly")) + 
  labs(y = "fit") +
  ggtitle("CGaussian with sinusoidal noise function, 10^5 data points") # loess is under loess.as
# at this point basically, loess does the same as loess.as, loess.as is great, locpoly underfits a lot (too big bw most likely)

# fitting into a table 

tab <- matrix(c(R2Lo3, R2Lp3, R2Loas3, R2Loly3, R2Lo4, R2Lp4, R2Loas4, R2Loly4,R2Lo5, "-", R2Loas5, R2Loly5, R2Lo6, "-", R2Loas6, R2Loly6), ncol=4, byrow=TRUE)
colnames(tab) <- c('Loess','locpol','loess.as',"locpoly")
rownames(tab) <- c('10^2','10^3','10^4',"10^5")
tab <- as.table(tab)

# plotting r^2 coefficients
locr2 <- c(R2Lo3,R2Lo4,R2Lo5,R2Lo6)
lpr2 <- c(R2Lp3,R2Lp4,0,0)
loasr2 <- c(R2Loas3,R2Loas4,R2Loas5,R2Loas6)
locyr2 <- c(R2Loly3,R2Loly4,R2Loly5,R2Loly6)
X <- c(10^2,10^3,10^4,10^5)

pr2c <- ggplot() + geom_point(mapping = aes(x = log10(X), y = locr2, color = "loess")) + 
  geom_point(mapping = aes(x = log10(X), y = lpr2, color = "locpol")) + 
  geom_point(mapping = aes(x = log10(X), y = loasr2, color = "loess.as")) + 
  geom_point(mapping = aes(x = log10(X), y = locyr2, color = "locpoly")) + 
  scale_color_manual(name = "Regression Function",
                     values = c( "loess" = "blue", "locpol" = "red", "loess.as" = "green", "locpoly" = "purple"),
                     labels = c("loess", "locpol", "loess.as","locpoly")) + 
  labs(y = "R2") +
  ggtitle("R2 coefficients applied to gaussian distribution with sinusoidal and gaussian noise, for different functions, and data size")
 pc4
 pc5
```

With so many data points and uniform noise, we have now good results for each of the three functions. We can see the regressions become more smooth as the number of data points goes up, most likely due to the lesser impact of the noise given, a direct consequence from the Law of Large Numbers. Now, let us have a final look at the evolution of the $R^2$ coefficients, although it is a worse indicator than checking the underlying fits.
```{r final results, echo = FALSE}
tab
pr2c
```

As we can see, when we increase the number of datapoints, the span and the bandwidth methods give similar results. For smaller samples however, the bandwidth approach is preferable. Again, we can see a clear overfitting tendency from the locpol function, as both of the considered coefficients of determination are equal to one, meaning the algorithm interpolated each point perfectly, which might be detrimental when working with noised data.

In summary, we observed two different approaches to local regression: one using a span parameter and one using a bandwidth, each with two different functions. It seems the bandwidth approach tends to underfit the data, especially when looking at small datasets, but works fine when dealing with larger samples. The bandwidth approach does not necessarily have this problem, but the two considered functions behave quite differently even in cases where they are givem identical parameters; the locpoly function is way more sensible to small fluctuations of the data, which is good to detect patterns with relatively small samples (order of 100,1000), but overfits quite badly when given noised data. The locpoly function on the contrary, tends to underfit on smaller samples (given the same bandwidth as the locpol function) but works well for bigger datasets. This is altogether an useful result, as we cannot apply the locpoly function on samples that are too big anyway.

A remaining concern is the difference in the results of the two functions taking a bandwidth as input parameter. Indeed, there is no explanation in the function descriptions that explain why they give back so different results, as both seem to 'fit a local polynomial given a kernel and a bandwidth'. A possibility would be the regression criteria, that could either be least squares, Aic, ...

# General Results and Discussion

# Conclusion



# References (add references to fcts and packages)