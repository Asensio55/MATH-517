---
title: 'Statistical Computation and Visualization: Final Project'
author: "Issam Arabi, Eric Maeder"
date: "2022-12-20"
output: html_document
bibliography: references.bib
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Comparison of Local Regression in different R packages.

# Introduction
When applying statistical methods in R using built functions, it is common to be confronted to multiple possible functions and packages aiming to tackle the same issues. In this scenario, and especially when dealing with large amounts of data, it is crucial yet difficult to choose the better optimized one towards fulfilling one's goals. Optimization denotes here both a measure of performance in the results and in the computation time.

In this context, we aim to explore the different R packages aiming to implement local polynomial interpolation and their performance by applying them and timing their execution on specifically chosen data sets of different sizes and different structures to properly assess their versatility.

# Plan
In this report, we will be following the structure from the work of  @deng2011density conducting a similar analysis on density estimation. Effectively, we will start with a small introduction to local polynomial regression, followed by the description of the different R packages we are to examine. Then, we will at the computation time of the different functions when applied to datasets of increasing sizes. Next, we will measure the performance of the considered functions when dealing with data sets with more challenging structures. Afterwards, we will discuss the results, and lastly, we will summarize our results and conclude.

# Local Polynomial Regression
Local Polyinomial regression, most widely known through the Locally Estimated Scatterplot Smoothing (LOESS) method, is a powerful prediction tool when observing bounded data sets with non-trivial distributions. It is a non-parametric method, which allows it to be versatile and easy to use. 

When observing response values $Y_1, Y_2, ...$ at points $X_1, X_2, ...$, the aim of local polynomial regression is to assign at each point X the value corresponding to $\underset{\beta \in \mathbb{R}^{p+1}}{argmin} [Y_i - \beta_0 - \beta_1(X_i - x) - ... - \beta_p (X_i - x)^p]K(\frac{X_i - x}{h})$, where p is the order of the fitted polynomial, K is a kernel function (basically a weighting function), and h is a bandwidth parameter allowing to choose how strong the influence of distant observations on the regression should be. 

Then, the resulting polynomials are combined and smoothed to obtain a curve spanning the whole considered region. In practice, polynomial degrees p < 4 are mostly used due to the increasingly computational challenge, which is also the approach we will adopt in this report.

# Local Polynomial Regression Packages

In this section, we give a quick overview of the considered functions and their parameters.

+ stats::loess, from its documentation, fits a locally polynomial surface determined by one or more numerical predictors, using local fitting. its typical call is as follow:

loess(formula, data, weights, subset, na.action, model = FALSE,
      span = 0.75, enp.target, degree = 2,
      parametric = FALSE, drop.square = FALSE, normalize = TRUE,
      family = c("gaussian", "symmetric"),
      method = c("loess", "model.frame"),
      control = loess.control(...), ...)
      
But in our case, we will be working on fully-specified data sets with no missing data and no prior information about value importance, so we will only use the following:

loess(formula, (data), 
      span = 0.75, degree = 2,
      family = "gaussian"
      method = "loess")
      
where we give our response and explanatory variables in formula, span controls the bandwidth, degree is self-explanatory and family specifies we want a least-squares approach. 
The span parameter works as follows: when given a value smaller than 1, each local regression is fitted using the proportion of observations equal to the span value into account, with tricubic weighting (1- (distance to point/maximal distance to considered points)^3)^3. 

This approach prevents cases of badly spread data where no observation is in the considered bandwidth when fitting a local regression, making it unstable, at the cost of computing the distance to each data point. Moreover, this makes it difficult to compare to other functions, as we cannot give them exactly the same inputs.

+ locpol::locpol, follows the steps of the local polynomial regression described in the previous section. Its call is as follows:

locpol(formula,data,weig=rep(1,nrow(data)),bw=NULL,kernel=EpaK,deg=1,
xeval=NULL,xevalLen=100)

again, we will not use all parameters for comparison in this report, although they offer room for versatility. Therefore, we will mostly use:

locpol(formula,(data),bw=NULL,kernel=EpaK,deg=1)

where formula, data and deg are identical to the loess function. The bw parameter is the bandwidth, and the kernel is the kernel function, Epanechnikov by default, but can be gaussian, uniform,...
This follows closely the theoretical approach we chose.

+ KernSmooth::locpoly aims to fit a local polynomial with kernel weighting to estimate a regression function. It can also be used towards density estimation and their derivatives. We call it as follows:

locpoly(x, y, drv = 0L, degree, kernel = "normal", 
        bandwidth, gridsize = 401L, bwdisc = 25, 
        range.x, binned = FALSE, truncate = TRUE)

Here, x,y are the explanatory and response variables, drv concerns derivative estimations, kernel and bandwidth are self-explanatory, gridsize represents the number of equidistant points over which we want to estimate the fct, and the last parameters are used to speed up computations when the data is pre-processed. This approach is pretty similar to locpol, and as such to our theoretical version. The similarity in input parameters makes it easy to compare both of them.

+ fANCOVA::loess.as performs local polynomial regression with automatic bandwidth selection using either cross-validation or AIC as criterion. It also has an option to select it manually, which we will use towards comparison. below the usage:

loess.as(x, y, degree = 1, criterion = c("aicc", "gcv"), 
		family = c("gaussian", "symmetric"), user.span = NULL, 
		plot = FALSE, ...)
		
where x,y,degree, criterion are self-explanatory. We will use here the gaussian family corresponding to the Least Squares approach, and user.span is a manual smoothing parameter selection (bandwidth value e.g.). plot is an option to generate a visualization of the results.

# comments (to do)
Add references to packages / fcts documentation in according section.

Comment on the outputs of each fct and what is calculated in the same section, as that might influence runtime.

# References (add references to fcts and packages)